name: Deploy Solutions

on:
  push:
    branches: [main]
  workflow_dispatch:

env:
  AWS_REGION: eu-north-1

jobs:
  deploy-containerized:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.5.0

      - name: Setup Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      # Enhanced Cleanup Phase
      - name: Initialize Terraform for Cleanup
        working-directory: containerized-solution/terraform
        run: terraform init

      - name: Remove Docker Stack
        working-directory: containerized-solution/terraform
        run: |
          MANAGER_IP=$(terraform output -raw manager_public_ip 2>/dev/null || echo "")
          if [ -n "$MANAGER_IP" ]; then
            echo "Removing Docker stack from swarm cluster..."
            ssh -o ConnectTimeout=10 -o StrictHostKeyChecking=no \
              -i ~/.ssh/docker-swarm-key.pem ec2-user@"$MANAGER_IP" \
              "docker stack rm myapp" || true
            sleep 30
          fi

      - name: Scale Down Auto Scaling Group
        working-directory: containerized-solution/terraform
        run: |
          ASG_NAME=$(terraform output -raw autoscaling_group_name 2>/dev/null || echo "")
          if [ -n "$ASG_NAME" ]; then
            echo "Scaling down ASG: $ASG_NAME"
            aws autoscaling update-auto-scaling-group \
              --auto-scaling-group-name "$ASG_NAME" \
              --min-size 0 --max-size 0 --desired-capacity 0 \
              --region ${{ env.AWS_REGION }} || true
            sleep 60
          fi

      - name: Cleanup S3 Bucket Completely
        working-directory: containerized-solution/terraform
        run: |
          S3_BUCKET=$(terraform output -raw s3_bucket_name 2>/dev/null || echo "")
          if [ -n "$S3_BUCKET" ]; then
            echo "Cleaning S3 bucket completely: $S3_BUCKET"
            
            # Remove all versioned objects
            aws s3api list-object-versions \
              --bucket "$S3_BUCKET" \
              --region ${{ env.AWS_REGION }} \
              --query 'Versions[].{Key:Key,VersionId:VersionId}' \
              --output text 2>/dev/null | while read key version_id; do
              if [ -n "$key" ] && [ -n "$version_id" ]; then
                aws s3api delete-object \
                  --bucket "$S3_BUCKET" \
                  --key "$key" \
                  --version-id "$version_id" \
                  --region ${{ env.AWS_REGION }} || true
              fi
            done
            
            # Remove all delete markers
            aws s3api list-object-versions \
              --bucket "$S3_BUCKET" \
              --region ${{ env.AWS_REGION }} \
              --query 'DeleteMarkers[].{Key:Key,VersionId:VersionId}' \
              --output text 2>/dev/null | while read key version_id; do
              if [ -n "$key" ] && [ -n "$version_id" ]; then
                aws s3api delete-object \
                  --bucket "$S3_BUCKET" \
                  --key "$key" \
                  --version-id "$version_id" \
                  --region ${{ env.AWS_REGION }} || true
              fi
            done
            
            # Remove remaining objects
            aws s3 rm "s3://$S3_BUCKET" --recursive --region ${{ env.AWS_REGION }} || true
            sleep 5
            
            # Delete bucket
            aws s3api delete-bucket --bucket "$S3_BUCKET" --region ${{ env.AWS_REGION }} || true
          fi

      - name: Destroy Existing Infrastructure
        working-directory: containerized-solution/terraform
        run: |
          echo "Destroying existing Terraform infrastructure..."
          terraform destroy -auto-approve || true

      - name: Clean ECR Repository
        run: |
          echo "Cleaning ECR repository: fastapi-upload-demo"
          aws ecr delete-repository \
            --repository-name fastapi-upload-demo \
            --region ${{ env.AWS_REGION }} \
            --force || true

      # Deploy Phase (same as before)
      - name: Deploy Infrastructure
        working-directory: containerized-solution/terraform
        run: |
          echo "Deploying new infrastructure..."
          terraform init
          terraform apply -auto-approve

      - name: Get Infrastructure Outputs
        working-directory: containerized-solution/terraform
        run: |
          echo "Retrieving infrastructure outputs..."
          echo "MANAGER_IP=$(terraform output -raw manager_public_ip)" >> $GITHUB_ENV
          echo "S3_BUCKET=$(terraform output -raw s3_bucket_name)" >> $GITHUB_ENV
          echo "DYNAMODB_TABLE=$(terraform output -raw dynamodb_table_name)" >> $GITHUB_ENV

      - name: Setup ECR Repository
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          echo "ECR_URI=${ACCOUNT_ID}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/fastapi-upload-demo" >> $GITHUB_ENV
          aws ecr create-repository --repository-name fastapi-upload-demo --region ${{ env.AWS_REGION }} || true

      - name: Login to ECR
        run: |
          aws ecr get-login-password --region ${{ env.AWS_REGION }} | docker login --username AWS --password-stdin ${{ env.ECR_URI }}

      - name: Build and Push Docker Image
        working-directory: containerized-solution/app
        run: |
          docker build -t ${{ env.ECR_URI }}:latest .
          docker push ${{ env.ECR_URI }}:latest

      - name: Setup SSH Key
        working-directory: containerized-solution
        run: |
          mkdir -p ~/.ssh
          cp terraform/docker-swarm-key.pem ~/.ssh/
          chmod 400 ~/.ssh/docker-swarm-key.pem

      - name: Deploy Application to Docker Swarm
        run: |
          ssh -o StrictHostKeyChecking=no -i ~/.ssh/docker-swarm-key.pem ec2-user@${{ env.MANAGER_IP }} "
            aws ecr get-login-password --region ${{ env.AWS_REGION }} | docker login --username AWS --password-stdin ${{ env.ECR_URI }}
            docker service create --name fastapi-app --mode global --publish 8001:8000 \
              --env AWS_REGION=${{ env.AWS_REGION }} \
              --env S3_BUCKET_NAME=${{ env.S3_BUCKET }} \
              --env DYNAMODB_TABLE_NAME=${{ env.DYNAMODB_TABLE }} \
              --with-registry-auth ${{ env.ECR_URI }}:latest || \
            docker service update --image ${{ env.ECR_URI }}:latest \
              --env-add AWS_REGION=${{ env.AWS_REGION }} \
              --env-add S3_BUCKET_NAME=${{ env.S3_BUCKET }} \
              --env-add DYNAMODB_TABLE_NAME=${{ env.DYNAMODB_TABLE }} \
              fastapi-app
          "

      - name: Display Deployment Information
        run: |
          echo "ğŸ‰ Containerized deployment completed!"
          echo "ğŸ“ Manager IP: ${{ env.MANAGER_IP }}"
          echo "ğŸª£ S3 Bucket: ${{ env.S3_BUCKET }}"
          echo "ğŸ—ƒï¸ DynamoDB Table: ${{ env.DYNAMODB_TABLE }}"

  deploy-serverless:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.5.0

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      # Enhanced Cleanup Phase following destroy.sh pattern
      - name: Initialize Terraform for Cleanup
        working-directory: serverless-solution/terraform
        run: terraform init

      - name: Disable CloudFront Distribution
        working-directory: serverless-solution/terraform
        run: |
          DIST_ID=$(terraform output -raw cloudfront_distribution_id 2>/dev/null || echo "")
          if [ -n "$DIST_ID" ]; then
            echo "Disabling CloudFront distribution: $DIST_ID"
            
            IS_ENABLED=$(aws cloudfront get-distribution \
              --id "$DIST_ID" \
              --query 'Distribution.DistributionConfig.Enabled' \
              --output text 2>/dev/null || echo "false")
            
            if [ "$IS_ENABLED" = "True" ]; then
              ETAG=$(aws cloudfront get-distribution-config \
                --id "$DIST_ID" \
                --query 'ETag' \
                --output text 2>/dev/null || echo "")
              
              if [ -n "$ETAG" ]; then
                aws cloudfront get-distribution-config \
                  --id "$DIST_ID" \
                  --query 'DistributionConfig' > /tmp/dist-config.json
                
                sed -i 's/"Enabled": true/"Enabled": false/g' /tmp/dist-config.json
                
                aws cloudfront update-distribution \
                  --id "$DIST_ID" \
                  --distribution-config file:///tmp/dist-config.json \
                  --if-match "$ETAG" || true
                
                rm -f /tmp/dist-config.json
                
                # Wait for deployment
                for i in {1..20}; do
                  STATUS=$(aws cloudfront get-distribution \
                    --id "$DIST_ID" \
                    --query 'Distribution.Status' \
                    --output text 2>/dev/null || echo "")
                  
                  if [ "$STATUS" = "Deployed" ]; then
                    break
                  fi
                  sleep 30
                done
              fi
            fi
          fi

      - name: Empty S3 Bucket Completely
        working-directory: serverless-solution/terraform
        run: |
          S3_BUCKET=$(terraform output -raw s3_bucket_name 2>/dev/null || echo "")
          if [ -n "$S3_BUCKET" ]; then
            echo "Emptying S3 bucket completely: $S3_BUCKET"
            
            # Check if bucket exists
            if aws s3api head-bucket --bucket "$S3_BUCKET" >/dev/null 2>&1; then
              # Remove all versioned objects
              aws s3api list-object-versions \
                --bucket "$S3_BUCKET" \
                --query 'Versions[].[Key,VersionId]' \
                --output text 2>/dev/null | while read key version_id; do
                
                if [ -n "$key" ] && [ "$key" != "None" ]; then
                  aws s3api delete-object \
                    --bucket "$S3_BUCKET" \
                    --key "$key" \
                    --version-id "$version_id" || true
                fi
              done
              
              # Remove all delete markers
              aws s3api list-object-versions \
                --bucket "$S3_BUCKET" \
                --query 'DeleteMarkers[].[Key,VersionId]' \
                --output text 2>/dev/null | while read key version_id; do
                
                if [ -n "$key" ] && [ "$key" != "None" ]; then
                  aws s3api delete-object \
                    --bucket "$S3_BUCKET" \
                    --key "$key" \
                    --version-id "$version_id" || true
                fi
              done
              
              # Remove remaining objects
              aws s3 rm "s3://$S3_BUCKET" --recursive || true
              sleep 5
            fi
          fi

      - name: Destroy Existing Infrastructure
        working-directory: serverless-solution/terraform
        run: |
          echo "Destroying existing Terraform infrastructure..."
          terraform destroy -auto-approve || true

      # Deploy Phase (same as before)
      - name: Deploy Infrastructure
        working-directory: serverless-solution/terraform
        run: |
          echo "Deploying new serverless infrastructure..."
          terraform init
          terraform apply -auto-approve

      - name: Get Infrastructure Outputs
        working-directory: serverless-solution/terraform
        run: |
          echo "S3_BUCKET=$(terraform output -raw s3_bucket_name)" >> $GITHUB_ENV
          echo "API_GATEWAY_URL=$(terraform output -raw api_gateway_url)" >> $GITHUB_ENV
          echo "CONTACT_API_URL=$(terraform output -raw contact_api_url)" >> $GITHUB_ENV
          echo "CLOUDFRONT_DISTRIBUTION_ID=$(terraform output -raw cloudfront_distribution_id)" >> $GITHUB_ENV

      - name: Build Static Website
        working-directory: serverless-solution/app
        run: |
          mkdir -p /tmp/serverless-build
          cp -r static /tmp/serverless-build/
          sed "s|{{ api_gateway_url }}|${{ env.API_GATEWAY_URL }}|g; s|{{ contact_api_url }}|${{ env.CONTACT_API_URL }}|g" templates/index.html > /tmp/serverless-build/index.html

      - name: Deploy Website to S3
        run: |
          aws s3 sync /tmp/serverless-build/ s3://${{ env.S3_BUCKET }}/ --delete

      - name: Invalidate CloudFront Cache
        run: |
          aws cloudfront create-invalidation --distribution-id ${{ env.CLOUDFRONT_DISTRIBUTION_ID }} --paths "/*" || true

      - name: Cleanup Build Files
        run: rm -rf /tmp/serverless-build

      - name: Display Deployment Information
        run: |
          echo "ğŸ‰ Serverless deployment completed!"
          echo "ğŸŒ API Gateway URL: ${{ env.API_GATEWAY_URL }}"
          echo "ğŸ“§ Contact API URL: ${{ env.CONTACT_API_URL }}"
          echo "ğŸª£ S3 Bucket: ${{ env.S3_BUCKET }}"
          echo "â˜ï¸ CloudFront Distribution: ${{ env.CLOUDFRONT_DISTRIBUTION_ID }}"
